{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "bank_data = pd.read_csv('bank-additional-full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Handling categorical variables\n",
    "\n",
    "job = pd.get_dummies(bank_data['job'], drop_first = True)\n",
    "marital = pd.get_dummies(bank_data['marital'], drop_first = True)\n",
    "education = pd.get_dummies(bank_data['education'], drop_first = True)\n",
    "default = pd.get_dummies(bank_data['default'], drop_first = True)\n",
    "housing = pd.get_dummies(bank_data['housing'], drop_first = True)\n",
    "loan =  pd.get_dummies(bank_data['loan'], drop_first = True)\n",
    "contact = pd.get_dummies(bank_data['contact'], drop_first = True)\n",
    "month = pd.get_dummies(bank_data['month'], drop_first = True)\n",
    "day_of_week = pd.get_dummies(bank_data['day_of_week'], drop_first = True)\n",
    "poutcome = pd.get_dummies(bank_data['poutcome'], drop_first = True)\n",
    "\n",
    "#reduced set of features that is going to be concatenated to existing ones\n",
    "bank_data_red = pd.concat([job,marital,education,default,housing,loan,contact,month,day_of_week,poutcome], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transformed bank_data set\n",
    "bank_data_tran = bank_data_red.join(bank_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bank_data_tran.drop(['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome'], axis = 1, inplace = True)\n",
    "bank_data_tran.drop(['duration'], axis = 1, inplace = True)\n",
    "\n",
    "#because of the high correlation coefficient between euribor3m rate and nr. of employed, the latter is going to be dropped\n",
    "bank_data_tran.drop(['nr.employed'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Data preprocessing and splitting and scaling\n",
    "x = bank_data_tran.iloc[:,bank_data_tran.columns != 'y'].values\n",
    "y = bank_data_tran.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Downsampled\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = bank_data_tran[bank_data_tran.y=='no']\n",
    "df_minority = bank_data_tran[bank_data_tran.y=='yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=5000,     # to match minority class\n",
    "                                 random_state=123) # reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     5000\n",
       "yes    4640\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_downsampled.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Data preprocessing and splitting and scaling\n",
    "x_down = df_downsampled.iloc[:,df_downsampled.columns != 'y'].values\n",
    "y_down = df_downsampled.iloc[:, -1].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_down, y_down, test_size = 0.2, random_state = 0)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.69      0.87      0.77       771\n",
      "        yes       0.83      0.61      0.70       772\n",
      "\n",
      "avg / total       0.76      0.74      0.74      1543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression Algorithm\n",
    "\n",
    "logdown = LogisticRegression()\n",
    "logdown.fit(x_train, y_train)\n",
    "predictions_LR = logdown.predict(x_val)\n",
    "\n",
    "print(classification_report(y_val,predictions_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set predictions: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.69      0.82      0.75       771\n",
      "        yes       0.78      0.62      0.69       772\n",
      "\n",
      "avg / total       0.73      0.72      0.72      1543\n",
      "\n",
      "Test predictions:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         no       0.72      0.81      0.76       981\n",
      "        yes       0.77      0.67      0.72       947\n",
      "\n",
      "avg / total       0.74      0.74      0.74      1928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classification Algorithm\n",
    "\n",
    "# Fitting Random Forest Classification to the Training set\n",
    "classifier = RandomForestClassifier(n_estimators = 40, criterion = 'gini', random_state = 0)\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "predictions_RF_val = classifier.predict(x_val)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#cm = confusion_matrix(y_val, predictions_RF)\n",
    "\n",
    "print(\"Validation set predictions: \\n\\n\" + classification_report(y_val,predictions_RF_val))\n",
    "\n",
    "predictions_RF_test = classifier.predict(x_test)\n",
    "\n",
    "print(\"Test predictions:\\n\\n \" + classification_report(y_test, predictions_RF_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set predictions: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.68      0.88      0.77       771\n",
      "        yes       0.83      0.59      0.69       772\n",
      "\n",
      "avg / total       0.75      0.73      0.73      1543\n",
      "\n",
      "Test set predictions:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.70      0.86      0.77       981\n",
      "        yes       0.81      0.62      0.70       947\n",
      "\n",
      "avg / total       0.75      0.74      0.74      1928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM algorithm\n",
    "\n",
    "svm_classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "svm_classifier.fit(x_train, y_train)\n",
    "\n",
    "predictions_SVC_val = svm_classifier.predict(x_val)\n",
    "print(\"Validation set predictions: \\n\\n\" + classification_report(y_val,predictions_SVC_val))\n",
    "\n",
    "predictions_SVC_test = svm_classifier.predict(x_test)\n",
    "print(\"Test set predictions:\\n\\n\" + classification_report(y_test, predictions_SVC_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set predictions: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.66      0.84      0.74       771\n",
      "        yes       0.78      0.58      0.66       772\n",
      "\n",
      "avg / total       0.72      0.71      0.70      1543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#KNN Algorithm\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=10, metric = 'minkowski', p = 2)\n",
    "knn_classifier.fit(x_train, y_train)\n",
    "\n",
    "predictions_KNN_val = knn_classifier.predict(x_val)\n",
    "print(\"Validation set predictions: \\n\\n\" + classification_report(y_val,predictions_KNN_val))\n",
    "\n",
    "#predictions_KNN_test = knn_classifier.predict(x_test)\n",
    "#print(\"Test set predictions:\\n\\n\" + classification_report(y_test, predictions_KNN_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set predictions: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.66      0.69      0.67       771\n",
      "        yes       0.67      0.64      0.66       772\n",
      "\n",
      "avg / total       0.66      0.66      0.66      1543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision tree algorithm\n",
    "\n",
    "td_classifier = DecisionTreeClassifier(criterion='entropy', random_state = 0)\n",
    "td_classifier.fit(x_train, y_train)\n",
    "\n",
    "predictions_DT_val = td_classifier.predict(x_val)\n",
    "print(\"Validation set predictions: \\n\\n\" + classification_report(y_val,predictions_DT_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
